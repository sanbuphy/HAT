# Configuration for multi-GPU training using accelerate for Real_HAT_SRx4 with MSE loss

name: train_Real_HAT_SRx4_mse_model
model: Real_HAT_SRx4
loss: mse
accelerate:
  use_multi_gpu: true
  num_processes: 4  # Adjust this based on the number of GPUs available
  mixed_precision: fp16  # Use mixed precision for faster training
  gradient_accumulation_steps: 1  # Adjust this based on your batch size and GPU memory

datasets:
  train:
    name: DIV2K
    type: ImageFolder
    dataroot: /path/to/DIV2K/train
    resolution: 4x
    augmentations:
      - type: RandomCrop
        size: 128
      - type: RandomHorizontalFlip
      - type: RandomVerticalFlip
  val:
    name: Set5
    type: ImageFolder
    dataroot: /path/to/Set5
    resolution: 4x

optimizer:
  type: Adam
  lr: 0.0001
  betas: [0.9, 0.999]
  weight_decay: 0.0001

scheduler:
  type: StepLR
  step_size: 200000
  gamma: 0.5

training:
  epochs: 1000
  batch_size: 16
  num_workers: 8
  save_checkpoint_freq: 10
  log_freq: 100
  eval_freq: 1000
  save_best_model: true
